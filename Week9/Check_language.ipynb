{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import find_keyword\n",
    "from pythainlp.util import rank\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import heapq\n",
    "from keybert import KeyBERT\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thai:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "        self.keyword = self.get_keyword()\n",
    "        self.summarize = self.get_summarize()\n",
    "    def make_sentence(self,list_word):\n",
    "        self.sentence_value = ''\n",
    "        for i in list_word:\n",
    "            for i in list_word:\n",
    "                if pythainlp.util.countthai(i)<10:\n",
    "                    list_word.remove(i)\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        return self.sentence_result\n",
    "    def get_keyword(self):\n",
    "        self.keyword_result = {}\n",
    "        self.keyword_value = word_tokenize(self.sentence, engine=\"newmm\")\n",
    "        self.keyword_dict = find_keyword(self.keyword_value)\n",
    "        # Iterate over the keys in the dictionary\n",
    "        for key in self.keyword_dict:\n",
    "        # Check if the key is text (i.e., not a space or quotation mark)\n",
    "            if key.isalpha():\n",
    "            # If the key is text, add it to the new dictionary\n",
    "                self.keyword_result[key] = self.keyword_dict[key]\n",
    "        return self.keyword_result\n",
    "    def get_summarize(self):\n",
    "        self.summarize_result =[]\n",
    "        self.summarize_result = summarize(self.sentence,n=5)\n",
    "        return self.summarize_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  try:\n",
    "    title_tag = soup.find('title').text\n",
    "  except:\n",
    "    title_tag = soup.find('title')\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "  for p in p_tags:\n",
    "    if p.string != None:\n",
    "      p_list.append(unicodedata.normalize(\"NFKD\", p.string))\n",
    "  if len(p_list) == 0:\n",
    "    p_list.append('ไม่พบข้อความในเว็บนี้')\n",
    "  \n",
    "  p_tag = \"\".join(p_list)\n",
    "  thai_nlp = Thai(p_list)\n",
    "  keyword = thai_nlp.keyword\n",
    "  keyword = {k: v for k, v in sorted(keyword.items(), key=lambda item: item[1], reverse=True)}\n",
    "  keyword = dict(itertools.islice(keyword.items(), 5))\n",
    "  summarize_article = thai_nlp.summarize\n",
    "  try:\n",
    "    location = 'จ.'+max(thai_nlp.get_location().keys())\n",
    "  except:\n",
    "    location = 'NONE'\n",
    "  \n",
    "  if  title_tag == None:\n",
    "    try:\n",
    "      title_tag = summarize_article[0]\n",
    "    except:\n",
    "      title_tag = 'NONE'\n",
    "    \n",
    "  return p_tag, title_tag, keyword,location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check language function\n",
    "def check_lang(url:str):\n",
    "    data_lang = scrape_tags(url)\n",
    "    percent = pythainlp.util.countthai(data_lang[0])\n",
    "    if percent >50:\n",
    "        return print(\"Thai language\")\n",
    "    else:\n",
    "        return print(\"English language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language\n"
     ]
    }
   ],
   "source": [
    "check_lang('https://www.bbc.com/afrique/media/photogalleries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai language\n"
     ]
    }
   ],
   "source": [
    "check_lang('https://www.thairath.co.th/news/crime/2633816')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6dbe31fb58d2a925cb6cf70124d8310e9312b8d1336a5bee8b6adf692fc3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
