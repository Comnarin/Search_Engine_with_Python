{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import find_keyword\n",
    "from pythainlp.util import rank\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url, depth=0, visited={}):\n",
    "  headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "  response = requests.get(url,headers=headers)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  links = soup.find_all('a')\n",
    "  links = [link.get('href') for link in links if link.get('href') and not link.get('href').startswith('#')]\n",
    "  links = [link for link in links if link.startswith(url) or link.startswith('/')]\n",
    "  links = [urljoin(url, link) for link in links if link]\n",
    "\n",
    "\n",
    "  # Recursively crawl the links at the next depth level\n",
    "  if depth < 3: #3\n",
    "    new_links = []\n",
    "    for link in links:\n",
    "      # Increment the visit count for the link\n",
    "      if link in visited:\n",
    "        visited[link] += 1\n",
    "      else:\n",
    "        visited[link] = 1\n",
    "      # Get the newly-crawled links and add them to the list\n",
    "        new_links.extend(get_all_links(link, depth=depth+1, visited=visited))\n",
    "      # Add the newly-crawled links to the original list\n",
    "    links.extend(new_links)\n",
    "\n",
    "  return visited\n",
    "\n",
    "base_url = 'https://www.thairath.co.th'\n",
    "website_dict = get_all_links(base_url, depth=0, visited={})\n",
    "print(website_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thai:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "        self.keyword = self.get_keyword()\n",
    "        self.summarize = self.get_summarize()\n",
    "    def make_sentence(self,list_word):\n",
    "        self.sentence_value = ''\n",
    "        for i in list_word:\n",
    "            for i in list_word:\n",
    "                if pythainlp.util.countthai(i)<10:\n",
    "                    list_word.remove(i)\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        return self.sentence_result\n",
    "    def get_keyword(self):\n",
    "        self.keyword_result = {}\n",
    "        self.keyword_value = word_tokenize(self.sentence, engine=\"newmm\")\n",
    "        self.keyword_dict = find_keyword(self.keyword_value)\n",
    "        # Iterate over the keys in the dictionary\n",
    "        for key in self.keyword_dict:\n",
    "        # Check if the key is text (i.e., not a space or quotation mark)\n",
    "            if key.isalpha():\n",
    "            # If the key is text, add it to the new dictionary\n",
    "                self.keyword_result[key] = self.keyword_dict[key]\n",
    "        return self.keyword_result\n",
    "    def get_summarize(self):\n",
    "        self.summarize_result =[]\n",
    "        self.summarize_result = summarize(self.sentence,n=5)\n",
    "        return self.summarize_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  title_tag = soup.find('title').text\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "  for p in p_tags:\n",
    "    if p.string != None:\n",
    "      p_list.append(unicodedata.normalize(\"NFKD\", p.string))\n",
    "  if len(p_list) == 0:\n",
    "    p_list.append('ไม่พบข้อความในเว็บนี้')\n",
    "  \n",
    "  p_tag = \"\".join(p_list)\n",
    "  thai_nlp = Thai(p_list)\n",
    "  keyword = thai_nlp.keyword\n",
    "  keyword = {k: v for k, v in sorted(keyword.items(), key=lambda item: item[1], reverse=True)}\n",
    "  keyword = dict(itertools.islice(keyword.items(), 5))\n",
    "  summarize_article = thai_nlp.summarize\n",
    "  \n",
    "  if  title_tag == None:\n",
    "    title_tag = summarize_article[0]\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  return p_tag, title_tag, keyword\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('scraped_data.db')\n",
    "conn.execute('''CREATE TABLE DATA\n",
    "             (ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             WEBSITE STRING NOT NULL,\n",
    "             BODY TEXT NOT NULL,\n",
    "             TITLE TEXT NOT NULL,\n",
    "             KEYWORD TEXT NOT NULL,\n",
    "             WORD_FREQUENCY INT NOT NULL,\n",
    "             REF INT NOT NULL);''')\n",
    "for website in website_dict.keys():\n",
    "  if 'news' in website:\n",
    "    p_tag, title, keyword  = scrape_tags(website)\n",
    "    for i in keyword: \n",
    "      conn.execute(\"INSERT INTO DATA (WEBSITE, BODY, TITLE, KEYWORD, WORD_FREQUENCY ,REF) VALUES (?, ?, ?, ?, ?, ?)\", (website, p_tag, title, i, keyword[i],website_dict[website]))\n",
    "      print(f'For website {website}\\n the p tags is: {p_tag} \\n the title tag is: {title}\\n  the keyword is:{i}\\n  the word frequency is:{keyword[i]}\\n  the ref is:{website_dict[website]}')\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "\n",
    "# Connect to the .db file\n",
    "conn = sqlite3.connect('scraped_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a Tkinter window\n",
    "root = Tk()\n",
    "root.title(\"Search\")\n",
    "\n",
    "# Create a style for the widgets\n",
    "style = ttk.Style()\n",
    "style.configure('.', font=('Arial', 24))\n",
    "\n",
    "# Create a label for the title\n",
    "title_label = ttk.Label(root, text=\"Search data\", style='Title.TLabel')\n",
    "title_label.pack()\n",
    "\n",
    "# Create a Frame for the input\n",
    "input_frame = ttk.Frame(root)\n",
    "input_frame.pack()\n",
    "\n",
    "# Create a StringVar to store the user input\n",
    "user_input = StringVar()\n",
    "\n",
    "# Create an Entry widget for the user to input text\n",
    "entry = ttk.Entry(input_frame, textvariable=user_input)\n",
    "entry.grid(row=0, column=0)\n",
    "\n",
    "# Create a button to submit the input\n",
    "submit_button = ttk.Button(input_frame, text='Submit', command = lambda: submit_query())\n",
    "submit_button.grid(row=0, column=1)\n",
    "\n",
    "# Create a Listbox widget to display the results\n",
    "listbox = Listbox(root, height=40, width=90)\n",
    "listbox.pack()\n",
    "\n",
    "def submit_query():\n",
    "    # Get user input\n",
    "    user_input_value = user_input.get()\n",
    "    # Clear the Listbox\n",
    "    listbox.delete(0, END)\n",
    "    # Execute the SQL query with user input\n",
    "    query = \"SELECT DISTINCT website,title,ref FROM DATA WHERE TITLE like ? order by ref desc\"\n",
    "    cursor.execute(query, ('%'+user_input_value+'%',))\n",
    "    # Fetch the results\n",
    "    results = cursor.fetchall()\n",
    "    # Iterate through the rows of the results\n",
    "    for row in results:\n",
    "        # Iterate through the columns of the current row\n",
    "        for column in row:\n",
    "            # Insert the column value into the Listbox\n",
    "            listbox.insert(END, column)\n",
    "        listbox.insert(END, '\\n')\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f77b640396ee907328d3f2b1dbf7d1073670d4b49ce003bfc4c9fbcbcb50868c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
