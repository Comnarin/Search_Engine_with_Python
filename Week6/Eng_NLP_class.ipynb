{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eng:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        return self.sentence_result\n",
    "    def make_sentence(self,list_word):\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def tokenize(self):\n",
    "        self.sentence_list = nltk.sent_tokenize(self.sentence)\n",
    "        return self.sentence_list\n",
    "    def create_formatted_article(self):\n",
    "        # Removing special characters and digits\n",
    "        self.formatted_article_text = re.sub('[^a-zA-Z]', ' ', self.sentence )\n",
    "        self.formatted_article_text = re.sub(r'\\s+', ' ', self.formatted_article_text)\n",
    "        return self.formatted_article_text\n",
    "    def find_word_frequencies(self):\n",
    "        self.tokenize()\n",
    "        self.create_formatted_article()\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.word_frequencies = {}\n",
    "        for word in nltk.word_tokenize(self.formatted_article_text):\n",
    "            if word not in stopwords:\n",
    "                if word not in self.word_frequencies.keys():\n",
    "                    self.word_frequencies[word] = 1\n",
    "                else:\n",
    "                    self.word_frequencies[word] += 1\n",
    "        return self.word_frequencies\n",
    "    def find_maximum_frequncy(self):\n",
    "        self.find_word_frequencies()\n",
    "        maximum_frequncy = max(self.word_frequencies.values())\n",
    "        for word in self.word_frequencies.keys():\n",
    "            self.word_frequencies[word] = (self.word_frequencies[word]/maximum_frequncy)\n",
    "    def find_sentence_scores(self):\n",
    "        self.sentence_scores = {}\n",
    "        for sent in self.sentence_list:\n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in self.word_frequencies.keys():\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in self.sentence_scores.keys():\n",
    "                            self.sentence_scores[sent] = self.word_frequencies[word]\n",
    "                        else:\n",
    "                            self.sentence_scores[sent] += self.word_frequencies[word]\n",
    "        return self.sentence_scores\n",
    "    def summary_sentences(self):\n",
    "        self.find_sentence_scores()\n",
    "        self.summary_sentences = heapq.nlargest(5, self.sentence_scores, key=self.sentence_scores.get)\n",
    "        summary = ' '.join(self.summary_sentences)\n",
    "        return self.summary_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6dbe31fb58d2a925cb6cf70124d8310e9312b8d1336a5bee8b6adf692fc3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
