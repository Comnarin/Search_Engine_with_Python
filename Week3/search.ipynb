{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese people have rushed to book overseas travel after Beijing announced it would reopen its borders next month.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28088/2325264678.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Retrieve all paragraphs and combine it as one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'ssrcss-7uxr49-RichTextContainer e5tfeyi1'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0msen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Make a request to the website\n",
    "r = requests.get('https://www.bbc.com')\n",
    "# Create an object to parse the HTML format\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# Retrieve all popular news links (Fig. 1)\n",
    "link = []\n",
    "for i in soup.find('div', {'class':\"module__content\"}).find_all('a'):\n",
    "    i['href'] = \"https://www.bbc.com/\"+i['href'] + '?page=all'\n",
    "    link.append(i['href'])\n",
    "\n",
    "# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\n",
    "documents = []\n",
    "for i in link:\n",
    "    # Make a request to the link\n",
    "    r = requests.get(i)\n",
    "  \n",
    "    # Initialize BeautifulSoup object to parse the content \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "  \n",
    "    # Retrieve all paragraphs and combine it as one\n",
    "    sen = []\n",
    "    for i in soup.find('div', {'class':'ssrcss-7uxr49-RichTextContainer e5tfeyi1'}).find_all('p'):\n",
    "        sen.append(i.text)\n",
    "  \n",
    "    # Add the combined paragraphs to documents\n",
    "    documents.append(' '.join(sen))\n",
    "    print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "documents_clean = []\n",
    "for d in documents:\n",
    "    # Remove Unicode\n",
    "    document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "    # Remove Mentions\n",
    "    document_test = re.sub(r'@\\w+', '', document_test)\n",
    "    # Lowercase the document\n",
    "    document_test = document_test.lower()\n",
    "    # Remove punctuations\n",
    "    document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "    # Lowercase the numbers\n",
    "    document_test = re.sub(r'[0-9]', '', document_test)\n",
    "    # Remove the doubled space\n",
    "    document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "    documents_clean.append(document_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abimanyu</th>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.028643</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absen</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>0.024495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020418</td>\n",
       "      <td>0.026721</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adalah</th>\n",
       "      <td>0.044485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.030402</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adanya</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yang</th>\n",
       "      <td>0.185570</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.098436</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.034588</td>\n",
       "      <td>0.100139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yong</th>\n",
       "      <td>0.073485</td>\n",
       "      <td>0.144721</td>\n",
       "      <td>0.247777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167404</td>\n",
       "      <td>0.058160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yunani</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zilky</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5  \\\n",
       "abimanyu  0.067871  0.028643  0.026406  0.000000  0.000000  0.000000   \n",
       "absen     0.000000  0.000000  0.000000  0.038035  0.000000  0.000000   \n",
       "ada       0.024495  0.000000  0.000000  0.020418  0.026721  0.023005   \n",
       "adalah    0.044485  0.000000  0.017307  0.000000  0.024264  0.020889   \n",
       "adanya    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "yang      0.185570  0.028477  0.039380  0.098436  0.018403  0.110907   \n",
       "year      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "yong      0.073485  0.144721  0.247777  0.000000  0.160328  0.000000   \n",
       "yunani    0.000000  0.000000  0.000000  0.000000  0.000000  0.042854   \n",
       "zilky     0.000000  0.000000  0.000000  0.000000  0.049776  0.000000   \n",
       "\n",
       "                 6         7         8         9  \n",
       "abimanyu  0.000000  0.000000  0.000000  0.000000  \n",
       "absen     0.000000  0.000000  0.000000  0.000000  \n",
       "ada       0.025800  0.000000  0.000000  0.029080  \n",
       "adalah    0.000000  0.014957  0.030402  0.026406  \n",
       "adanya    0.000000  0.000000  0.000000  0.054170  \n",
       "...            ...       ...       ...       ...  \n",
       "yang      0.017769  0.011344  0.034588  0.100139  \n",
       "year      0.000000  0.030683  0.000000  0.000000  \n",
       "yong      0.000000  0.000000  0.167404  0.058160  \n",
       "yunani    0.000000  0.000000  0.000000  0.000000  \n",
       "zilky     0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[1140 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "X = vectorizer.fit_transform(documents_clean)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: absen\n",
      "Berikut artikel dengan nilai cosine similarity tertinggi: \n",
      "Nilai Similaritas: 0.03803504597824992\n",
      "kompas com agen karim benzema karim djaziri mengungkit kembali kontroversi seputar masalah yang dihadapi kliennya sehingga dipaksa meninggalkan timnas perancis dan gagal ikut piala dunia sang agen mengunggah gambar yang membuktikan karim benzema seharusnya masih bisa berperan membantu perancis dalam putaran final turnamen empat tahunan tersebut yang sudah rampung bergulir pada desember ada banyak rumor komentar dan laporan tentang cara benzema dipulangkan dari kamp perancis sebelum kick off piala dunia baca juga drama benzema unfollow pemain timnas perancis kecuali sosok ini awalnya benzema cedera saat latihan bersama tim kemudian striker real madrid tersebut diklaim sulit pulih dalam waktu singkat sehingga pelatih didier deschamps memutuskan memulangkan benzema kini djaziri meluruskan tentang cedera yang dialami benzema dia menyebut cedera peraih ballon d or tersebut tidak parah sehingga masih bisa bermain dalam piala dunia djaziri mengungkapkan hal tersebut melalui unggahan di twitter soal jangka waktu pemulihan cedera benzema yang diperkuat dengan unggahan melalui video je pose a l mais avant a j ai consult sp cialistes qui confirment le diagnostic que aurait pu tre apte partir des me pour au moins tre sur le banc pourquoi lui avoir demand de partir si vite pic twitter com wtohhdedvw saya berkonsultasi dengan tiga spesialis yang mengonfirmasi bahwa benzema bisa saja fit sejak babak besar setidaknya cukup untuk berada di bangku cadangan demikian cuitan djaziri mengapa anda memintanya pergi begitu cepat sang agen menyuarakan apa yang menjadi pertanyaan publik karena deschamps dinilai terlalu buru buru memulangkan benzema baca juga karim benzema pensiun dari timnas perancis dia menegaskan cedera benzema tak serius dengan demikian striker tahun itu masih bisa masuk skuad meskipun absen selama fase penyisihan grup tetapi bisa berkontribusi dalam babak knock out memang saat mengalami cedera benzema diprediksi pulih dalam waktu tiga minggu sejak november terbukti benzema sudah bisa liburan dan fit ketika perancis melawan polandia pada desember namun deschamps sudah membuat keputusan dia memulangkan benzema tanpa pernah menggunakan jasa sang striker kontroversi ini menjadi perbincangan dan kisah tersebut ditutup dengan keputusan benzema pensiun dari timnas perancis beberapa saat setelah negaranya kalah dari argentina dalam final piala dunia \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_similar_articles(q, df):\n",
    "  import numpy as np\n",
    "  print(\"query:\", q)\n",
    "  print(\"Berikut artikel dengan nilai cosine similarity tertinggi: \")\n",
    "    # Convert the query become a vector\n",
    "  q = [q]\n",
    "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "  sim = {}\n",
    "    # Calculate the similarity\n",
    "  for i in range(10):\n",
    "     sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "    \n",
    "    # Sort the values \n",
    "  sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Print the articles and their similarity values\n",
    "  for k, v in sim_sorted:\n",
    "    if v != 0.0:\n",
    "      print(\"Nilai Similaritas:\", v)\n",
    "      print(documents_clean[k])        \n",
    "      print()\n",
    "  # Add The Query\n",
    "\n",
    "\n",
    "q1 = input(str)\n",
    "  # Call the function\n",
    "get_similar_articles(q1, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6dbe31fb58d2a925cb6cf70124d8310e9312b8d1336a5bee8b6adf692fc3cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
