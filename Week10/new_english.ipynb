{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neither spaCy nor NLTK have any methods for filtering punctuations \n",
    "def remove_punctuations(normalized_tokens):\n",
    "    punctuations=['?',':','!',',','.',';','|','(',')','--','\\n']\n",
    "    for word in normalized_tokens:\n",
    "        if word in punctuations:\n",
    "            normalized_tokens.remove(word)  \n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_text(doc:str):\n",
    "    vocab = English()\n",
    "    # Create a Tokenizer with the default settings for English\n",
    "    tokenizer = vocab.tokenizer\n",
    "    tokens = tokenizer(doc)\n",
    "    # just keeping pos tagger and lemmatizer\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner',\n",
    "                                            'tok2vec', 'attribute_ruler'])\n",
    "    doc = nlp(doc)   \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    normalized_tokens =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            normalized_tokens.append(word) \n",
    "    normalized_tokens = remove_punctuations(normalized_tokens)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nouns(text):\n",
    "    nlp2 = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "    nouns = []\n",
    "    for word in text:\n",
    "        doc_nouns = nlp2(word)\n",
    "        for token in doc_nouns:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"VERB\":\n",
    "                nouns.append(token.text)\n",
    "    return nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xi', 'jinping', 'set', 'deepen', 'control', 'china', 'government', 'economy', 'lawmakers', 'meet', 'beijing', 'pass', 'far', '-', 'reaching', 'reforms', '\\n\\n', 'national', 'people', 'congress', 'npc', 'rubber', '-', 'stamp', 'parliament', 'confirm', 'mr', 'xi', 'term', 'president', 'appointments', 'team', '\\n\\n', 'new', 'premier', 'second', '-', '-', 'command', 'mr', 'xi', 'incumbent', 'li', 'keqiang', 'departs', '\\n\\n', 'sessions', 'meetings', 'known', 'annual', 'affair', '\\n\\n', 'year', 'sessions', 'particularly', 'significant', 'delegates', 'expected', 'reshape', 'key', 'communist', 'party', 'state', 'institutions', '\\n\\n', 'tighten', 'control', 'bodies', 'overseeing', 'finance', 'sector', 'scientific', 'technology', 'work', '\"', 'strengthening', 'party', '-', 'building', 'work', '\"', 'private', 'businesses', 'according', 'state', 'media', '\\n\\n', 'moves', 'likely', 'blur', 'lines', 'chinese', 'communist', 'party', 'government', ',', 'consolidate', 'party', 'control', 'private', 'sector', '\\n\\n', 'comes', 'amid', 'ongoing', 'corruption', 'crackdown', 'seen', 'string', 'high', '-', 'profile', 'businessmen', 'disappear', 'recent', 'years', 'latest', 'person', 'missing', 'china', 'dealmakers', 'tech', 'sector']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "#nlp2 = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "word_list = normalized_text('''Xi Jinping is set to deepen his control of China's government and economy, as lawmakers meet in Beijing to pass far-reaching reforms.\n",
    "\n",
    "The National People's Congress (NPC), a rubber-stamp parliament, will confirm Mr Xi's third term as president, and the appointments of his top team.\n",
    "\n",
    "They will also name a new premier, the second-in-command after Mr Xi, as the incumbent Li Keqiang departs.\n",
    "\n",
    "The Two Sessions, as the meetings are known, are an annual affair.\n",
    "\n",
    "But this year's sessions are particularly significant as delegates are expected to reshape several key Communist Party and state institutions.\n",
    "\n",
    "They will also tighten control over bodies overseeing the finance sector and scientific and technology work, while \"strengthening party-building work\" in private businesses, according to state media.\n",
    "\n",
    "The moves will likely further blur the lines between the Chinese Communist Party and the government, and consolidate the party's control of the private sector.\n",
    "\n",
    "This comes amid an ongoing corruption crackdown which has seen a string of high-profile businessmen disappear in recent years. The latest person to go missing was one of China's top dealmakers in the tech sector.''')\n",
    "\n",
    "#nouns = []\n",
    "#for word in word_list:\n",
    "#   doc_nouns = nlp2(word)\n",
    "#   for token in doc_nouns:\n",
    "#      if token.pos_ == \"NOUN\":\n",
    "#          nouns.append(token.text)\n",
    "#print(nouns)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(keyword_text:str):\n",
    "    normalize = normalized_text(keyword_text)\n",
    "    return normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jeng', 'eating', 'friedrice', 'ping', 'eating', 'noodle']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eating', 'friedrice', 'eating', 'noodle']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = search(\"Jeng is eating friedrice and Ping is eating noodle\")\n",
    "print(x)\n",
    "find_nouns(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
