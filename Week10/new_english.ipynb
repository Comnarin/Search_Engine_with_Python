{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neither spaCy nor NLTK have any methods for filtering punctuations \n",
    "def remove_punctuations(normalized_tokens):\n",
    "    punctuations=['?',':','!',',','.',';','|','(',')','--','\\n']\n",
    "    for word in normalized_tokens:\n",
    "        if word in punctuations:\n",
    "            normalized_tokens.remove(word)  \n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_text(doc:str):\n",
    "    vocab = English()\n",
    "    # Create a Tokenizer with the default settings for English\n",
    "    tokenizer = vocab.tokenizer\n",
    "    tokens = tokenizer(doc)\n",
    "    # just keeping pos tagger and lemmatizer\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner',\n",
    "                                            'tok2vec', 'attribute_ruler'])\n",
    "    doc = nlp(doc)   \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    normalized_tokens =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            normalized_tokens.append(word) \n",
    "    normalized_tokens = remove_punctuations(normalized_tokens)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nouns(text):\n",
    "    nlp2 = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "    nouns = []\n",
    "    for word in text:\n",
    "        doc_nouns = nlp2(word)\n",
    "        for token in doc_nouns:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"VERB\":\n",
    "                nouns.append(token.text)\n",
    "    return nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_list = '''Xi Jinping is set to deepen his control of China's government and economy, as lawmakers meet in Beijing to pass far-reaching reforms.\n",
    "\n",
    "The National People's Congress (NPC), a rubber-stamp parliament, will confirm Mr Xi's third term as president, and the appointments of his top team.\n",
    "\n",
    "They will also name a new premier, the second-in-command after Mr Xi, as the incumbent Li Keqiang departs.\n",
    "\n",
    "The Two Sessions, as the meetings are known, are an annual affair.\n",
    "\n",
    "But this year's sessions are particularly significant as delegates are expected to reshape several key Communist Party and state institutions.\n",
    "\n",
    "They will also tighten control over bodies overseeing the finance sector and scientific and technology work, while \"strengthening party-building work\" in private businesses, according to state media.\n",
    "\n",
    "The moves will likely further blur the lines between the Chinese Communist Party and the government, and consolidate the party's control of the private sector.\n",
    "\n",
    "This comes amid an ongoing corruption crackdown which has seen a string of high-profile businessmen disappear in recent years. The latest person to go missing was one of China's top dealmakers in the tech sector.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def spacy_process(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "#Tokenization and lemmatization \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    #print(\"Tokenize+Lemmatize:\")\n",
    "    #print(lemma_list)\n",
    "    \n",
    "    #Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    #Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    #print(\" \")\n",
    "    #3print(\"Remove stopword & punctuation: \")\n",
    "    #print(filtered_sentence)\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeng', 'eat', 'friedrice', 'Ping', 'eat', 'noodle', 'Moss', 'eat', 'apple']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Jeng is eating friedrice and Ping is eating noodle and Moss has ate apple\"\n",
    "spacy_process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Narin', 'Sirinapuk', 'study', 'Software', 'Development']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Narin Sirinapuk is studying Software Development.\"\n",
    "spacy_process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    def process(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Tokenization and lemmatization \n",
    "        lemma_list = []\n",
    "        for token in doc:\n",
    "            lemma_list.append(token.lemma_)\n",
    "        \n",
    "        # Filter the stopword\n",
    "        filtered_sentence =[] \n",
    "        for word in lemma_list:\n",
    "            lexeme = self.nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence.append(word) \n",
    "\n",
    "        # Remove punctuation\n",
    "        punctuations=\"?:!.,;\"\n",
    "        filtered_sentence = [word for word in filtered_sentence if word not in punctuations]\n",
    "\n",
    "        return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
