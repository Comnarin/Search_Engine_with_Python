{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import find_keyword\n",
    "from pythainlp.util import rank\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from pythainlp.tag import tag_provinces\n",
    "from datetime import datetime\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spyder:\n",
    "    def __init__( self ,target_links,base_url ):\n",
    "        self.base_url = base_url\n",
    "        self.target_links = target_links\n",
    "    \n",
    "    def get_crawler(self):\n",
    "        self.result_crawler = self.crawl(self.base_url,0,set())\n",
    "        return self.result_crawler\n",
    "    \n",
    "    def get_check_domain(self):\n",
    "        self.check_domain_result = self.check_domain(self.base_url,self.get_crawler())\n",
    "        return self.check_domain_result\n",
    "    \n",
    "    def get_check_not_domain(self):\n",
    "        self.check_not_domain_result = self.check_not_domain(self.base_url,self.get_crawler())   \n",
    "        return self.check_not_domain_result\n",
    "    \n",
    "    def get_check_ref(self):\n",
    "        self.check_ref_result = self.check_ref(self.get_check_not_domain(),self.target_links)\n",
    "        return self.check_ref_result\n",
    "    \n",
    "    def get_all(self):\n",
    "        crawl = self.crawl(self.base_url,0,set())\n",
    "        check_domain =  self.check_domain(self.base_url,crawl) \n",
    "        check_not_domain = self.check_not_domain(self.base_url,crawl)\n",
    "        check_ref = self.check_ref(check_not_domain,self.target_links)\n",
    "        return check_domain,check_ref\n",
    "    \n",
    "    def crawl(self,url, depth,visited):\n",
    "        if depth < 3 :\n",
    "            visited.add(url)\n",
    "            headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "            time.sleep(0.3)\n",
    "            response = requests.get(url,headers=headers)\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            except:\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "            links = soup.find_all('a')\n",
    "            links = [link.get('href') for link in links if link.get('href') and not link.get('href').startswith('#')]\n",
    "            links = [urljoin(url, link) for link in links if link]\n",
    "\n",
    "            for link in links:\n",
    "                if link not in visited:\n",
    "                    link = link.replace(' ','')\n",
    "                    visited.add(link)\n",
    "                    if link.startswith(url):\n",
    "                        self.crawl(link,depth=depth+1, visited=visited)\n",
    "        return visited\n",
    "    \n",
    "    def check_domain(self,base_url,links):\n",
    "        result= set()\n",
    "        for link in links :\n",
    "            if link.startswith(base_url):\n",
    "                result.add(link)\n",
    "        return result\n",
    "    \n",
    "    def check_not_domain(self,base_url,links):\n",
    "        result= set()\n",
    "        for link in links :\n",
    "            if not link.startswith(base_url):\n",
    "                result.add(link)\n",
    "        return result\n",
    "    \n",
    "    def check_ref(self,links,target_links):\n",
    "        for i in links:\n",
    "            for j in target_links:\n",
    "                if i.startswith(j):\n",
    "                    target_links[j]+=1\n",
    "        return target_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  try:\n",
    "    title_tag = soup.find('title').text\n",
    "  except:\n",
    "    title_tag = soup.find('title')\n",
    "  body_tag = soup.find('body')\n",
    "  text_below_body = body_tag.get_text() \n",
    "  body_list =[]\n",
    "  body_list.append(text_below_body)\n",
    "  return (body_list,title_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(body):\n",
    "    for i in body:\n",
    "        output = i.replace('\\n', '  ').replace('\\xa0', '  ').replace('Â®', ' ').replace(';', ' ')\n",
    "        output = \" \".join(output.split())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neither spaCy nor NLTK have any methods for filtering punctuations \n",
    "def remove_punctuations(normalized_tokens):\n",
    "    punctuations=['?',':','!',',','.',';','|','(',')','--','\\n']\n",
    "    for word in normalized_tokens:\n",
    "        if word in punctuations:\n",
    "            normalized_tokens.remove(word)  \n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_text(doc:str):\n",
    "    vocab = English()\n",
    "    # Create a Tokenizer with the default settings for English\n",
    "    tokenizer = vocab.tokenizer\n",
    "    tokens = tokenizer(doc)\n",
    "    # just keeping pos tagger and lemmatizer\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner',\n",
    "                                            'tok2vec', 'attribute_ruler'])\n",
    "    doc = nlp(doc)   \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    normalized_tokens =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            normalized_tokens.append(word) \n",
    "    normalized_tokens = remove_punctuations(normalized_tokens)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(body):\n",
    "    words = normalized_text(body)\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc(link,target_links):\n",
    "    print(link)\n",
    "    link.replace(\" \", \"\")\n",
    "    d=dict()    \n",
    "    body, title = scrape_tags(link)\n",
    "    body=cleansing(body)\n",
    "    word = get_word(body)\n",
    "    d['link']= link\n",
    "    d['title'] = title\n",
    "    d['body']=body\n",
    "    d['location']='location'\n",
    "    d['word'] = word\n",
    "    for k in target_links:\n",
    "        if link.startswith(k):\n",
    "            d['ref'] = target_links[k]\n",
    "    print(d)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(target_links):\n",
    "    doc=[]\n",
    "    num=0\n",
    "    for i in target_links:\n",
    "        web_spyder=spyder(target_links,i)\n",
    "        domain_links,target_links =web_spyder.get_all()\n",
    "        print('all link =', len(domain_links))\n",
    "        for j in domain_links:\n",
    "            num+=1\n",
    "            d = make_doc(j,target_links)\n",
    "            doc.append(d)\n",
    "            print(num)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_links = {'https://www.bbc.com/news':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=get_doc(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite3 database\n",
    "conn = sqlite3.connect('inverted_index2.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for words, documents, and word frequencies\n",
    "conn.execute('''\n",
    "CREATE TABLE words (\n",
    "    ID INTEGER PRIMARY KEY,\n",
    "    Word TEXT NOT NULL UNIQUE\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.execute('''\n",
    "CREATE TABLE documents (\n",
    "    ID INTEGER PRIMARY KEY,\n",
    "    Link TEXT NOT NULL UNIQUE ,\n",
    "    Title TEXT,\n",
    "    Body TEXT,\n",
    "    Location TEXT,\n",
    "    Ref INTEGER,\n",
    "    Time TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.execute('''\n",
    "CREATE TABLE word_frequencies (\n",
    "    Word_ID INTEGER ,\n",
    "    Doc_ID INTEGER ,\n",
    "    Frequency INTEGER NOT NULL,\n",
    "    TF_IDF REAL ,\n",
    "    PRIMARY KEY (word_id, doc_id),\n",
    "    FOREIGN KEY (word_id) REFERENCES words(id),\n",
    "    FOREIGN KEY (doc_id) REFERENCES documents(id)\n",
    ");\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tf_idf():\n",
    "    conn = sqlite3.connect('inverted_index2.db',timeout=3)\n",
    "\n",
    "    cursor = conn.execute('SELECT COUNT(*) FROM documents')\n",
    "    N = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor = conn.execute('SELECT ID, Word FROM words')\n",
    "    words = cursor.fetchall()\n",
    "    \n",
    "    for word in words:\n",
    "        word_id = word[0]\n",
    "        word_str = word[1]\n",
    "\n",
    "        cursor = conn.execute('SELECT Doc_ID, Frequency FROM word_frequencies WHERE Word_ID = ?', (word_id,))\n",
    "        doc_freqs = cursor.fetchall()\n",
    "\n",
    "        df = len(doc_freqs)\n",
    "        idf = math.log(N / df)\n",
    "\n",
    "        for doc_freq in doc_freqs:\n",
    "            doc_id = doc_freq[0]\n",
    "            tf = doc_freq[1]\n",
    "            tf_idf = tf * idf\n",
    "            conn.execute('UPDATE word_frequencies SET TF_IDF = ? WHERE Word_ID = ? AND Doc_ID = ?', (tf_idf, word_id, doc_id))\n",
    "\n",
    "    conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_database(doc):\n",
    "  conn = sqlite3.connect('inverted_index2.db')\n",
    "  for i in doc:\n",
    "    conn.execute('''INSERT INTO documents (Link, Title, Body, Location, Ref,Time) VALUES (?, ?, ?, ?, ?, ?);''', (i['link'], i['title'], i['body'],i['location'],i['ref'],datetime.now()))\n",
    "    doc_id = conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "    \n",
    "    for j in i['word'].keys():\n",
    "      word_id = conn.execute(\"SELECT id FROM words WHERE word = ?\", (j,)).fetchone()\n",
    "      if not word_id:\n",
    "        conn.execute(\"INSERT INTO words (word) VALUES (?)\", (j,))\n",
    "        word_id = conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "      else:\n",
    "        word_id = word_id[0]\n",
    "      \n",
    "      conn.execute('''INSERT INTO word_frequencies (word_id, doc_id, Frequency) VALUES (?, ?, ?);''', (word_id, doc_id, i['word'][j]))\n",
    "  \n",
    " \n",
    "    \n",
    "  conn.commit()\n",
    "  update_tf_idf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data(link):\n",
    "    conn = sqlite3.connect('inverted_index2.db', timeout=10)\n",
    "\n",
    "    # retry up to 3 times if the database is locked\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            doc_id = conn.execute('''\n",
    "                SELECT id FROM documents WHERE link = ?; ''', (link,)).fetchone()[0]\n",
    "            conn.execute('''\n",
    "                DELETE FROM documents WHERE link = ?; ''', (link,))\n",
    "\n",
    "            conn.execute('''\n",
    "                DELETE FROM word_frequencies WHERE Doc_ID = ?;''', (doc_id,))\n",
    "\n",
    "            conn.execute('''\n",
    "                DELETE FROM words\n",
    "                WHERE NOT EXISTS (SELECT 1 FROM word_frequencies WHERE word_frequencies.word_id = words.id );''')\n",
    "            conn.commit()\n",
    "            update_tf_idf()\n",
    "            break  # exit the loop if commit is successful\n",
    "\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if 'database is locked' in str(e):\n",
    "                print('Database is locked, retrying...')\n",
    "                time.sleep(1)  # wait for 1 second before retrying\n",
    "            else:\n",
    "                raise e  # raise the error if it's not a locking issue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_to_database(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data(link):\n",
    "    conn = sqlite3.connect('inverted_index2.db',timeout=10)\n",
    "    doc_id = conn.execute('''\n",
    "    SELECT id FROM documents WHERE link = ?; ''', (link,)).fetchone()[0]\n",
    "    conn.execute('''\n",
    "        DELETE FROM documents WHERE link = ?; ''', (link,))\n",
    "\n",
    "    conn.execute('''\n",
    "        DELETE FROM word_frequencies WHERE Doc_ID = ?;''', (doc_id,))\n",
    "\n",
    "    conn.execute('''\n",
    "        DELETE FROM words\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM word_frequencies WHERE word_frequencies.word_id = words.id );''')\n",
    "    \n",
    "    conn.commit()\n",
    "    update_tf_idf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_data('https://www.bbc.com/news/uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(target_links):\n",
    "    conn = sqlite3.connect('inverted_index2.db')\n",
    "    for i in target_links:\n",
    "        get_link = spyder(target_links,i)\n",
    "        domain_link,target_links = get_link.get_all()\n",
    "    for j in domain_link:\n",
    "        link = conn.execute('''SELECT  documents.link\n",
    "                                    FROM documents\n",
    "                                    WHERE documents.link = ?\n",
    "                                    ''',(j,)) \n",
    "        link = link.fetchone()\n",
    "        doc = [make_doc(j,target_links)]\n",
    "        if link == None :\n",
    "            print(j)\n",
    "            insert_to_database(doc)\n",
    "        else:\n",
    "            print(j)\n",
    "            delete_data(j)\n",
    "            insert_to_database(doc)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_links = {'https://www.bbc.com/news':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_data(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(search_term):\n",
    "    conn = sqlite3.connect('inverted_index2.db')\n",
    "    results = conn.execute('''\n",
    "        SELECT documents.link, documents.title\n",
    "        FROM documents\n",
    "        JOIN word_frequencies ON word_frequencies.doc_id = documents.id\n",
    "        JOIN words ON words.id = word_frequencies.word_id\n",
    "        WHERE words.word LIKE ?\n",
    "        GROUP BY documents.id\n",
    "        ORDER BY SUM(word_frequencies.TF_IDF) DESC;\n",
    "    ''', ('%'+search_term+'%',))\n",
    "    print('Search Term:',search_term)\n",
    "    print('Search results:')\n",
    "    result = results.fetchone()\n",
    "    if result is not None:\n",
    "        while result is not None:\n",
    "            print(result)\n",
    "            result = results.fetchone()\n",
    "    else:\n",
    "        print('not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = str(input()) \n",
    "search(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = sqlite3.connect('inverted_index2.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT Link FROM documents')\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def update_backlinks():\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "    conn = sqlite3.connect('inverted_index2.db')\n",
    "\n",
    "    # Get all the documents\n",
    "    documents = conn.execute('SELECT Link FROM documents').fetchall()\n",
    "\n",
    "    # Loop over each document\n",
    "    for url in documents:\n",
    "        # Parse the HTML body and extract all links\n",
    "        \n",
    "        response = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        backlinks = [link['href'] for link in links]\n",
    "\n",
    "        # Loop over each backlink and update the count\n",
    "        for backlink in backlinks:\n",
    "            # Remove any anchors or query strings from the link\n",
    "            backlink = re.sub(r'[#?].*$', '', backlink)\n",
    "\n",
    "            # Check if the backlink already exists in the database\n",
    "            row = conn.execute('SELECT Count FROM backlinks WHERE Link = ?', (backlink,)).fetchone()\n",
    "\n",
    "            if row:\n",
    "                # Update the count\n",
    "                count = row[0] + 1\n",
    "                conn.execute('UPDATE backlinks SET Count = ? WHERE Link = ?', (count, backlink))\n",
    "            else:\n",
    "                # Insert a new row\n",
    "                conn.execute('INSERT INTO backlinks (Link, Count) VALUES (?, ?)', (backlink, 1))\n",
    "\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute('''CREATE TABLE backlinks (\n",
    "    ID INTEGER PRIMARY KEY,\n",
    "    Link TEXT NOT NULL UNIQUE ,\n",
    "    Count INTEGER NOT NULL);'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_backlinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = str(input())\n",
    "sentence = ['an elephant is eating banana and swiming in a Pool']\n",
    "clean_sentence = cleansing(sentence)\n",
    "word = normalized_text(clean_sentence)\n",
    "print(clean_sentence)\n",
    "print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".lower().split()\n",
    "an elephant is eating banana and swiming in a Pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f77b640396ee907328d3f2b1dbf7d1073670d4b49ce003bfc4c9fbcbcb50868c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
