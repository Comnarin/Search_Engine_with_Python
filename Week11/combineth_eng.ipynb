{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import find_keyword\n",
    "from pythainlp.util import rank\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import heapq\n",
    "from keybert import KeyBERT\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thai:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "        self.summarize = self.get_summarize()\n",
    "        self.word = self.get_word() \n",
    "    def make_sentence(self,list_word):\n",
    "        self.sentence_value = ''\n",
    "        for i in list_word:\n",
    "            for i in list_word:\n",
    "                if pythainlp.util.countthai(i)<10:\n",
    "                    list_word.remove(i)\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        return self.sentence_result\n",
    "    def get_word(self):\n",
    "        self.word_value = word_tokenize(self.sentence, engine=\"newmm\")\n",
    "        # Iterate over the keys in the dictionary\n",
    "        return self.word_value\n",
    "    def get_summarize(self):\n",
    "        self.summarize_result =[]\n",
    "        self.summarize_result = summarize(self.sentence,n=5)\n",
    "        return self.summarize_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  try:\n",
    "    title_tag = soup.find('title').text\n",
    "  except:\n",
    "    title_tag = soup.find('title')\n",
    "  try:\n",
    "    body_tag = soup.find('body')\n",
    "    text_below_body = body_tag.get_text().lower()\n",
    "  except:\n",
    "    text_below_body = 'not found'\n",
    "  body_list =[]\n",
    "  body_list.append(text_below_body)\n",
    "  return (body_list,title_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lang(url:str):\n",
    "    data_lang = scrape_tags(url)\n",
    "    percent = pythainlp.util.countthai(data_lang[0][0])\n",
    "    if percent >50:\n",
    "        thai_nlp = Thai(data_lang[0]) \n",
    "        a= thai_nlp.word\n",
    "        new_list = [s.strip().replace('\"', '') for s in a if s.strip()]\n",
    "        while '' in new_list:\n",
    "            new_list.remove('')\n",
    "\n",
    "    else:\n",
    "        return print(\"English language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language\n"
     ]
    }
   ],
   "source": [
    "check_lang('https://www.bbc.com/afrique/media/photogalleries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ไทยรัฐ', 'สำนักข่าว', 'อันดับ', '1', 'ของ', 'ไทย', '|', 'ไทยรัฐ', 'ออนไลน์', 'ไทยรัฐ', 'ออนไลน์', 'ข่าว', 'พระ', 'ราชสำนัก', 'ทั่ว', 'ไทย', 'ใน', 'กระแส', 'การเมือง', 'เศรษฐกิจ', 'ต่างประเทศ', 'อาชญากรรม', 'ยานยนต์', 'เทคโนโลยี', 'ราคา', 'ทองคำ', 'ราย', 'งานพิเศษ', 'วิดีโอ', 'หนังสือพิมพ์', 'ไทยรัฐ', 'ทีวี', 'ดู', 'ย้อนหลัง', 'ผัง', 'รายการ', 'live', 'ไลฟ์สไตล์', 'กีฬา', 'ฟุตบอล', 'ต่างประเทศ', 'ฟุตบอล', 'ไทย', 'sport', 'insider', 'ไฟต์', 'สปอร์ต', 'กีฬา', 'โลก', 'วิดีโอ', 'แกลเลอรี่', 'ฟุตบอล', 'โลก', '2022', 'บันเทิง', 'ดวง', 'หวย', 'นิยาย', 'โปรโมชั่น', 'ซื้อ', '-', 'ขาย', 'ส่วนลด', 'เช็ค', 'ราคา', 'thairathplus', 'ยิง', 'ดับ', 'พล.ต.ท.', 'ปัญญา', 'ผบช.น.', 'ยัน', 'ฝีมือ', 'ภรรยา', 'อยู่', 'ใน', 'อาการ', 'ช็อก', 'ยัง', 'ให้การ', 'ไม่', 'ได้', 'livelive', 'เข้าสู่ระบบ', 'สมัคร', 'สมาชิก', 'ค้นหา', 'hamburgerhamburger', 'ค้นหา', 'thairath', 'membership', 'เข้าสู่ระบบ', 'สมัคร', 'สมาชิก', 'live', 'ชม', 'รายการ', 'สด', 'ข่าว', 'back', 'ข่าว', 'ข่าว', 'ล่าสุด', 'พระ', 'ราชสำนัก', 'ทั่ว', 'ไทย', 'ใน', 'กระแส', 'การเมือง', 'เศรษฐกิจ', 'ต่างประเทศ', 'อาชญากรรม', 'ยานยนต์', 'เทคโนโลยี', 'ราคา', 'ทองคำ', 'ราย', 'งานพิเศษ', 'กีฬา', 'back', 'ฟุตบอล', 'ต่างประเทศ', 'ฟุตบอล', 'ไทย', 'sport', 'insider', 'ไฟต์', 'สปอร์ต', 'กีฬา', 'โลก', 'วิดีโอ', 'แกลเลอรี่', 'ฟุตบอล', 'โลก', '2022', 'ไทยรัฐ', 'ทีวี', 'back', 'ดู', 'ย้อนหลัง', 'วาไรตี้', 'บันเทิง', 'กีฬา', 'ผัง', 'รายการ', 'live', 'โปรโมชั่น', 'back', 'โปรโมชั่น', 'ซื้อ', '-', 'ขาย', 'ส่วนลด', 'เช็ค', 'ราคา', 'thairath', 'talk', 'หนังสือพิมพ์', 'คอลัมน์', 'บันเทิง', 'ดวง', 'หวย', 'นิยาย', 'วิดีโอ', 'podcast', 'ไลฟ์สไตล์', 'กิจกรรม', 'thairath', 'plusbackthairath', 'plusspeakmoneyeveryday', 'lifenature', 'mattersubculturefuturismsparkmirrorbackmirror', '100', 'ปี', 'ชาต', 'กาล', 'กำ', 'พล', 'วัชร', 'พล', 'ร่วมงาน', 'กับ', 'เรา', 'เกี่ยวกับ', 'เรา', 'ติดต่อ', 'เรา', 'ติดต่อ', 'โฆษณา', 'facebookfacebooktwittertwitterinstagraminstagramyoutubeyoutube', '19', 'ก.พ.', '2566', '17:32', 'น.', 'ข่าว', 'อาชญากรรม', 'ไทยรัฐ', 'ออนไลน์', 'ยิง', 'ดับ', 'พล.ต.ท.', 'ปัญญา', 'ผบช.น.', 'ยัน', 'ฝีมือ', 'ภรรยา', 'อยู่', 'ใน', 'อาการ', 'ช็อก', 'ยัง', 'ให้การ', 'ไม่', 'ได้', 'แชร์', 'ข่าว', 'แชร์', 'ข่าว', 'พล.ต.ท.', 'ธิติ', 'แสงสว่าง', 'ผบช.น.', 'เผย', 'มูลเหตุ', 'พล.ต.ท.', 'ปัญญา', 'ถูก', 'ยิง', 'เสียชีวิต', 'เป็นเรื่อง', 'ปัญหา', 'ความเครียด', 'ภายใน', 'ครอบครัว', 'คุมตัว', 'ผู้ก่อเหตุ', 'สอบปากคำ', 'ที่', 'สน.', 'ธรรม', 'ศาลา', 'ยังอยู่', 'ใน', 'อาการ', 'ช็อก', 'ไม่', 'สามารถ', 'ให้การ', 'ใดๆ', 'ได้', 'เมื่อ', 'เวลา', '17.50', 'น.', 'วันที่', '19', 'ก.พ.', '66', 'ผู้สื่อข่าว', 'รายงาน', 'ความคืบหน้า', 'กรณี', 'พล.ต.ท.', 'ปัญญา', 'ปิ่น', 'สุข', 'ผบ', 'ช.', 'ประจำ', 'สง', '.', 'ผบ.', 'ตร.', 'อายุ', '59', 'ปี', 'ถูก', 'ยิง', 'เสียชีวิต', 'หน้า', 'บ้านพัก', 'เลขที่', '278', 'ซอย', 'บรม', 'ฯ', '70', 'ถนน', 'บรม', 'ราช', 'ชนนี', 'แขวง', 'ศาลา', 'ธรรม', 'สพน์', 'เขต', 'ทวีวัฒนา', 'กรุงเทพฯ', 'โดย', 'ภรรยา', 'เป็น', 'ผู้', 'มา', 'พบ', 'ศพ', 'คน', 'แรก', 'โดย', 'ทาง', 'พล.ต.ท.', 'ธิติ', 'แสงสว่าง', 'ผบช.น.', 'อยู่', 'ระหว่าง', 'ตรวจสอบ', 'ที่เกิดเหตุ', 'เพื่อ', 'หา', 'ร่องรอย', 'หรือ', 'หลักฐาน', 'ทาง', 'คดี', 'เบื้องต้น', 'พบ', 'อาวุธ', 'ปืน', 'ตก', 'อยู่', 'ใน', 'ที่เกิดเหตุ', 'จำนวน', '2', 'กระบอก', 'เป็น', 'แม็กกาซีน', 'ยี่ห้อ', 'ซิก', 'ซาว', 'เออร์', '1', 'กระบอก', 'และ', 'ปืน', 'ลูกโม่', 'ไม่', 'ทราบ', 'ยี่ห้อ', '1', 'กระบอก', 'พล.ต.ท.', 'ธิติ', 'แสงสว่าง', 'ผบช.น.', 'เผย', 'ว่า', 'เป็นปัญหา', 'ความเครียด', 'ของ', 'คน', 'ภายใน', 'ครอบครัว', 'สำหรับ', 'ผู้ก่อเหตุ', 'คือ', 'ภรรยา', 'ของ', 'ผู้เสียชีวิต', 'ถูก', 'นำ', 'ตัว', 'ไป', 'สอบปากคำ', 'ที่', 'สน.', 'ธรรม', 'ศาลา', 'อยู่', 'ใน', 'อาการ', 'ช็อก', 'ยัง', 'ไม่', 'สามารถ', 'ให้การ', 'ใดๆ', 'ได้', 'สำหรับ', 'พล.ต.ท.', 'ปัญญา', 'ปิ่น', 'สุข', 'ผู้เสียชีวิต', 'ผบช.น.', 'เผย', 'ว่า', 'เจ้าตัว', 'เป็น', 'คน', 'นิสัย', 'ดี', 'น่ารัก', 'ใจเย็น', 'สุภาพ', 'เป็น', 'คน', 'มีความรู้', 'ขยัน', 'ทำงาน', 'ยืนยัน', 'ว่าไม่ได้', 'มีปัญหา', 'กับ', 'เพื่อน', 'ร่วม', 'งา', 'น.', 'sponsoredsponsored']\n"
     ]
    }
   ],
   "source": [
    "check_lang('https://www.thairath.co.th/news/crime/2633816')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_continuous_chunks(text,label):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk\n",
    "def eng_location(doc):\n",
    "    return get_continuous_chunks(doc, 'GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_location('India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
