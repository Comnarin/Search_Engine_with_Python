{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from pythainlp.tag import tag_provinces\n",
    "from pythainlp.tokenize import word_tokenize as tokenizer\n",
    "from datetime import datetime\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spyder:\n",
    "    def __init__( self ,links,base_url,depth ):\n",
    "        self.base_url = base_url\n",
    "        target_links={}\n",
    "        for i in links:\n",
    "            target_links[i]=0 \n",
    "        self.target_links = target_links\n",
    "        self.depth = depth\n",
    "    \n",
    "    def get_crawler(self):\n",
    "        self.result_crawler = self.crawl(self.base_url,self.depth,0,set())\n",
    "        return self.result_crawler\n",
    "    \n",
    "    def get_check_domain(self):\n",
    "        self.check_domain_result = self.check_domain(self.base_url,self.get_crawler())\n",
    "        return self.check_domain_result\n",
    "    \n",
    "    def get_check_not_domain(self):\n",
    "        self.check_not_domain_result = self.check_not_domain(self.base_url,self.get_crawler())   \n",
    "        return self.check_not_domain_result\n",
    "    \n",
    "    def get_check_ref(self):\n",
    "        self.check_ref_result = self.check_ref(self.get_check_not_domain(),self.target_links)\n",
    "        return self.check_ref_result\n",
    "    \n",
    "    def get_all(self):\n",
    "        crawl = self.crawl(self.base_url,self.depth,0,set())\n",
    "        check_domain =  self.check_domain(self.base_url,crawl) \n",
    "        check_not_domain = self.check_not_domain(self.base_url,crawl)\n",
    "        check_ref = self.check_ref(check_not_domain,self.target_links)\n",
    "        return check_domain,check_ref\n",
    "    \n",
    "    def crawl(self,url,n, depth,visited):\n",
    "        if depth < n :\n",
    "            visited.add(url)\n",
    "            headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "            time.sleep(0.3)\n",
    "            response = requests.get(url,headers=headers)\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            except:\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "            links = soup.find_all('a')\n",
    "            links = [link.get('href') for link in links if link.get('href') and not link.get('href').startswith('#')]\n",
    "            links = [urljoin(url, link) for link in links if link]\n",
    "\n",
    "            for link in links:\n",
    "                if link not in visited:\n",
    "                    link = link.replace(' ','')\n",
    "                    visited.add(link)\n",
    "                    if link.startswith(url):\n",
    "                        self.crawl(link,n=n,depth=depth+1, visited=visited)\n",
    "        return visited\n",
    "    \n",
    "    def check_domain(self,base_url,links):\n",
    "        result= set()\n",
    "        for link in links :\n",
    "            if link.startswith(base_url):\n",
    "                result.add(link)\n",
    "        return result\n",
    "    \n",
    "    def check_not_domain(self,base_url,links):\n",
    "        result= set()\n",
    "        for link in links :\n",
    "            if not link.startswith(base_url):\n",
    "                result.add(link)\n",
    "        return result\n",
    "    \n",
    "    def check_ref(self,links,target_links):\n",
    "        for i in links:\n",
    "            for j in target_links:\n",
    "                if i.startswith(j):\n",
    "                    target_links[j]+=1\n",
    "        return target_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor = conn.cursor()\n",
    "\n",
    "# cursor.execute(\"DROP TABLE Temp_Link;\")\n",
    "# cursor.execute(\"DROP TABLE Domain_Link;\")\n",
    "\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ref():\n",
    "    conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "    domain = conn.execute(\"SELECT domain_link FROM domain_link ;\").fetchall()\n",
    "    domain = [t[0] for t in domain]\n",
    "    for i in domain :\n",
    "        web = spyder(domain,i,1)\n",
    "        ref = web.get_check_ref()\n",
    "    check_link = conn.execute(\"SELECT link FROM documents ;\").fetchall()\n",
    "    check_link = [t[0] for t in check_link]\n",
    "    for j in check_link:\n",
    "        for k in ref:\n",
    "            if j.startswith(k):\n",
    "                conn.execute('UPDATE documents SET REF = ? WHERE link = ? ', (ref[k], j,))\n",
    "    \n",
    "update_ref()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawl_to_temp(target_links,db):\n",
    "    conn = sqlite3.connect(db)\n",
    "    for i in target_links:\n",
    "        domain = conn.execute(\"SELECT id FROM domain_link  WHERE domain_link  = ?\", (i,)).fetchone()\n",
    "        if not domain:\n",
    "            conn.execute(\"INSERT INTO domain_link (domain_link) VALUES (?)\", (i,))\n",
    "        web_spyder = spyder(target_links,i,1)\n",
    "        domainlinks  = web_spyder.get_check_domain()\n",
    "        for link in domainlinks:\n",
    "            conn.execute('''INSERT INTO Temp_link (Link) VALUES (?);''', (link,))\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thai:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "        self.summarize = self.get_summarize()\n",
    "        self.word = self.get_word() \n",
    "    def make_sentence(self,list_word):\n",
    "        list_word = [list_word]\n",
    "        self.sentence_value = ''\n",
    "        for i in list_word:\n",
    "            for i in list_word:\n",
    "                if pythainlp.util.countthai(i)<10:\n",
    "                    list_word.remove(i)\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        return self.sentence_result\n",
    "    def get_word(self):\n",
    "        self.word_value = tokenizer(self.sentence, engine=\"newmm\")\n",
    "        return self.word_value\n",
    "    def get_summarize(self):\n",
    "        self.summarize_result =[]\n",
    "        self.summarize_result = summarize(self.sentence,n=5)\n",
    "        return self.summarize_result\n",
    "    def location(self):\n",
    "        self.data = self.get_tokenize()\n",
    "        self.location_value = tag_provinces(self.data)\n",
    "        self.Result_location = [entry for entry in self.location_value if entry[1] == 'B-LOCATION']\n",
    "        return self.Result_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_process(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "#Tokenization and lemmatization \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    #print(\"Tokenize+Lemmatize:\")\n",
    "    #print(lemma_list)\n",
    "    \n",
    "    #Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    #Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    #print(\" \")\n",
    "    #3print(\"Remove stopword & punctuation: \")\n",
    "    #print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(body):\n",
    "    for i in body:\n",
    "        output = i.replace('\\n', '  ').replace('\\xa0', '  ').replace('Â®', ' ').replace(';', ' ')\n",
    "        output = \" \".join(output.split())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  try:\n",
    "    title_tag = soup.find('title').text\n",
    "  except:\n",
    "    title_tag = soup.find('title')\n",
    "  try:\n",
    "    body_tag = soup.find('body')\n",
    "    text_below_body = body_tag.get_text().lower()\n",
    "  except:\n",
    "    text_below_body = 'not found'\n",
    "  body_list =[]\n",
    "  body_list.append(text_below_body)\n",
    "  return (body_list,title_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(body):\n",
    "    word_freq = {}\n",
    "    for word in body:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as thaitokenize , pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_continuous_chunks(text,label):\n",
    "    chunked = ne_chunk(pos_tag(thaitokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    if continuous_chunk ==[]:\n",
    "        return 'None'\n",
    "\n",
    "    else:\n",
    "        return continuous_chunk\n",
    "\n",
    "def eng_location(doc):\n",
    "    return get_continuous_chunks(doc[0], 'GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lang(url:str):\n",
    "    data_lang,title = scrape_tags(url)\n",
    "    try:\n",
    "        percent = pythainlp.util.countthai(data_lang[0][0])\n",
    "        if percent >50:\n",
    "            thai_nlp = Thai(data_lang[0]) \n",
    "            word = thai_nlp.word\n",
    "            try:\n",
    "                location = 'à¸.'+max(thai_nlp.get_location().keys())\n",
    "            except:\n",
    "                location = 'Thailand'\n",
    "            new_list = [s.strip().replace('\"', '') for s in word if s.strip()]\n",
    "            while '' in new_list:\n",
    "                new_list.remove('')\n",
    "            word = get_word(new_list)\n",
    "            return data_lang,word,title,location\n",
    "        else:\n",
    "            clean_body=cleansing(data_lang)\n",
    "            body = spacy_process(cleansing(data_lang))\n",
    "            word = get_word(body)\n",
    "            location = eng_location(data_lang)\n",
    "            return clean_body,word,title,location\n",
    "    except:\n",
    "        clean_body=cleansing(data_lang)\n",
    "        body = spacy_process(cleansing(data_lang))\n",
    "        word = get_word(body)\n",
    "        location = eng_location(data_lang)\n",
    "        return clean_body,word,title,location\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_lang('https://www.thairath.co.th/news/crime/2633816')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_lang('https://www.bbc.com/news/world-asia-64957293')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc(link,target_links):\n",
    "    link.replace(\" \", \"\")\n",
    "    d=dict()\n",
    "    body,word,title,location=check_lang(link)\n",
    "    d['link']= link\n",
    "    d['title'] = title\n",
    "    d['body']=body\n",
    "    d['location']=location\n",
    "    d['word'] = word\n",
    "    for k in target_links:\n",
    "        if link.startswith(k):\n",
    "            d['ref'] = target_links[k]\n",
    "    print(d)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_links = {'http://www.bbc.com':0}\n",
    "for i in target_links:\n",
    "    make_doc(i,target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(links,n):\n",
    "    target_links=dict()\n",
    "    for i in links:\n",
    "        target_links[i]=0\n",
    "    doc=[]\n",
    "    num=0\n",
    "    for i in target_links:\n",
    "        web_spyder=spyder(target_links,i,n)\n",
    "        domain_links,target_links =web_spyder.get_all()\n",
    "        print('all link =', len(domain_links))\n",
    "        for j in domain_links:\n",
    "            num+=1\n",
    "            d = make_doc(j,target_links)\n",
    "            doc.append(d)\n",
    "            print(num)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_links=['https://www.bbc.com/news','https://edition.cnn.com','https://www.bangkokpost.com','http://www.thairath.co.th/news',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_doc(target_links,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite3 database\n",
    "conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for words, documents, and word frequencies\n",
    "\n",
    "conn.execute('''\n",
    "CREATE TABLE words (\n",
    "    ID INTEGER PRIMARY KEY,\n",
    "    Word TEXT NOT NULL UNIQUE\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.execute('''\n",
    "CREATE TABLE documents (\n",
    "    ID INTEGER PRIMARY KEY,\n",
    "    Link TEXT NOT NULL UNIQUE ,\n",
    "    Title TEXT,\n",
    "    Body TEXT,\n",
    "    Location TEXT,\n",
    "    Ref INT,\n",
    "    Time TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.execute('''\n",
    "CREATE TABLE word_frequencies (\n",
    "    Word_ID INTEGER ,\n",
    "    Doc_ID INTEGER ,\n",
    "    Frequency INTEGER NOT NULL,\n",
    "    TF_IDF REAL ,\n",
    "    PRIMARY KEY (word_id, doc_id),\n",
    "    FOREIGN KEY (word_id) REFERENCES words(id),\n",
    "    FOREIGN KEY (doc_id) REFERENCES documents(id)\n",
    ");\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tf_idf():\n",
    "    conn = sqlite3.connect('../Week10/inverted_index2.db',timeout=3)\n",
    "\n",
    "    cursor = conn.execute('SELECT COUNT(*) FROM documents')\n",
    "    N = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor = conn.execute('SELECT ID, Word FROM words')\n",
    "    words = cursor.fetchall()\n",
    "    \n",
    "    for word in words:\n",
    "        word_id = word[0]\n",
    "        word_str = word[1]\n",
    "\n",
    "        cursor = conn.execute('SELECT Doc_ID, Frequency FROM word_frequencies WHERE Word_ID = ?', (word_id,))\n",
    "        doc_freqs = cursor.fetchall()\n",
    "\n",
    "        df = len(doc_freqs)\n",
    "        idf = math.log(N / df)\n",
    "\n",
    "        for doc_freq in doc_freqs:\n",
    "            doc_id = doc_freq[0]\n",
    "            tf = doc_freq[1]\n",
    "            tf_idf = tf * idf\n",
    "            conn.execute('UPDATE word_frequencies SET TF_IDF = ? WHERE Word_ID = ? AND Doc_ID = ?', (tf_idf, word_id, doc_id))\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_database(doc):\n",
    "  conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "  for i in doc:\n",
    "    conn.execute('''INSERT INTO documents (Link, Title, Body, Location, Ref, Time) VALUES (?, ?, ?, ?, ?, ?);''', (str(i['link']), str(i['title']), str(i['body']), str(i['location']), int(i['ref']), datetime.now()))\n",
    "    doc_id = conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "    \n",
    "    for j in i['word'].keys():\n",
    "      word_id = conn.execute(\"SELECT id FROM words WHERE word = ?\", (j,)).fetchone()\n",
    "      if not word_id:\n",
    "        conn.execute(\"INSERT INTO words (word) VALUES (?)\", (j,))\n",
    "        word_id = conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "      else:\n",
    "        word_id = word_id[0]\n",
    "      \n",
    "      conn.execute('''INSERT INTO word_frequencies (word_id, doc_id, Frequency) VALUES (?, ?, ?);''', (word_id, doc_id, i['word'][j]))\n",
    "  \n",
    " \n",
    "    \n",
    "  conn.commit()\n",
    "  update_tf_idf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_to_database(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data(link):\n",
    "    db_dir = '../Week10/inverted_index2.db'\n",
    "    conn = sqlite3.connect(db_dir,timeout=10)\n",
    "    doc_id = conn.execute('''\n",
    "    SELECT id FROM documents WHERE link = ?; ''', (link,)).fetchone()[0]\n",
    "    conn.execute('''\n",
    "        DELETE FROM documents WHERE link = ?; ''', (link,))\n",
    "\n",
    "    conn.execute('''\n",
    "        DELETE FROM word_frequencies WHERE Doc_ID = ?;''', (doc_id,))\n",
    "\n",
    "    conn.execute('''\n",
    "        DELETE FROM words\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM word_frequencies WHERE word_frequencies.word_id = words.id );''')\n",
    "    \n",
    "    conn.commit()\n",
    "    update_tf_idf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'link':'wwww.','title':'asdasd','body':'asdasd',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(link):\n",
    "    db_dir = '../Week10/inverted_index2.db'\n",
    "    conn = sqlite3.connect(db_dir,timeout=10)\n",
    "    target_links={}\n",
    "    for i in link:\n",
    "        target_links[link]=0\n",
    "    for i in target_links:\n",
    "        get_link = spyder(target_links,i,2)\n",
    "        domain_link,target_links = get_link.get_all()\n",
    "    for j in domain_link:\n",
    "        link = conn.execute('''SELECT  documents.link\n",
    "                                    FROM documents\n",
    "                                    WHERE documents.link = ?\n",
    "                                    ''',(j,)) \n",
    "        link = link.fetchone()\n",
    "        doc = [make_doc(j,target_links)]\n",
    "        if link == None :\n",
    "            print(j)\n",
    "            insert_to_database(doc)\n",
    "        else:\n",
    "            print(j)\n",
    "            delete_data(j)\n",
    "            insert_to_database(doc)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_data('https://www.bbc.com/news',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [str(input())]\n",
    "clean_sentence = cleansing(sentence)\n",
    "word = spacy_process(clean_sentence)\n",
    "print(clean_sentence)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "    cursor = conn.cursor()\n",
    "    search_term = [str(input())]\n",
    "    clean_sentence = cleansing(search_term)\n",
    "    words = spacy_process(clean_sentence)\n",
    "    doc_lists = []\n",
    "    for word in words:\n",
    "        cursor.execute('SELECT Doc_ID FROM word_frequencies WHERE Word_ID = (SELECT ID FROM words WHERE Word = ?)', (word,))\n",
    "        doc_lists.append(set(doc[0] for doc in cursor.fetchall()))\n",
    "\n",
    "    # Find the documents that contain all the words in the sentence\n",
    "    matching_docs = set.intersection(*doc_lists)\n",
    "\n",
    "    # Calculate the score for each document\n",
    "    scores = {}\n",
    "    for doc_id in matching_docs:\n",
    "        score = sum(cursor.execute('SELECT Frequency FROM word_frequencies WHERE Word_ID = (SELECT ID FROM words WHERE Word = ?) AND Doc_ID = ?', (word, doc_id)).fetchone()[0] for word in words)\n",
    "        scores[doc_id] = score\n",
    "\n",
    "    # Sort the documents by score in descending order\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the list of document links\n",
    "    doc_links = []\n",
    "    for doc_id, score in sorted_docs:\n",
    "        cursor.execute('SELECT Link, title FROM documents WHERE ID = ?', (doc_id,))\n",
    "        doc_links.append(cursor.fetchone()[0])\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "    # Close the database connection and return the list of document links\n",
    "    conn.close()\n",
    "    print(search_term)\n",
    "    return doc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "\n",
    "def search_sentence():\n",
    "    conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "    cursor = conn.cursor()\n",
    "    search_term = str(input())\n",
    "    list_term = [search_term]\n",
    "    clean_sentence = cleansing(list_term)\n",
    "    words = spacy_process(clean_sentence)\n",
    "\n",
    "    # Retrieve the word IDs for each word in the sentence\n",
    "    word_ids = []\n",
    "    for word in words:\n",
    "        cursor.execute('SELECT word_ID FROM words WHERE word = ?', (word,))\n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            word_ids.append(result[0])\n",
    "\n",
    "    # Join the word_frequencies and documents tables to get the documents that contain any of the words in the sentence\n",
    "    cursor.execute('''\n",
    "        SELECT documents.Link, documents.Title, SUM(word_frequencies.TF_IDF)\n",
    "        FROM word_frequencies\n",
    "        JOIN documents ON documents.ID = word_frequencies.Doc_ID\n",
    "        WHERE word_frequencies.Word_ID IN ({})\n",
    "        GROUP BY documents.ID\n",
    "        ORDER BY SUM(word_frequencies.TF_IDF) DESC\n",
    "    '''.format(','.join(str(id) for id in word_ids)))\n",
    "\n",
    "    # Get the document links and titles for the top results\n",
    "    results = cursor.fetchall()\n",
    "    print('search term : ',search_term)\n",
    "    print('search result: ')\n",
    "    for result in results:\n",
    "        print(result[0], result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "\n",
    "def sentence_search():\n",
    "    conn = sqlite3.connect('../Week10/inverted_index2.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Split the query into individual words\n",
    "    search_term = [str(input())]\n",
    "    clean_sentence = cleansing(search_term)\n",
    "    words = spacy_process(clean_sentence)\n",
    "\n",
    "    # Retrieve the documents that contain each word\n",
    "    doc_lists = []\n",
    "    for word in words:\n",
    "        cursor.execute(\"SELECT Doc_ID, TF_IDF FROM word_frequencies JOIN words ON words.ID = word_frequencies.word_ID WHERE word = ?\", (word,))\n",
    "        doc_list = cursor.fetchall()\n",
    "        doc_lists.append(doc_list)\n",
    "\n",
    "    # Merge the document lists using the TF-IDF scores\n",
    "    doc_scores = {}\n",
    "    for doc_list in doc_lists:\n",
    "        for doc_id, tf_idf in doc_list:\n",
    "            if doc_id in doc_scores:\n",
    "                doc_scores[doc_id] += tf_idf\n",
    "            else:\n",
    "                doc_scores[doc_id] = tf_idf\n",
    "\n",
    "    # Rank the documents by their overall relevance\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Retrieve the links and titles of the top documents\n",
    "    results = []\n",
    "    for doc_id, score in ranked_docs:\n",
    "        cursor.execute(\"SELECT Link, Title FROM documents WHERE ID = ?\", (doc_id,))\n",
    "        link, title = cursor.fetchone()\n",
    "        results.append((link, title))\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =sentence_search()\n",
    "for i in a :\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
