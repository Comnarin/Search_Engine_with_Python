{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import unicodedata\n",
    "import pythainlp.util\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import find_keyword\n",
    "from pythainlp.util import rank\n",
    "#from pythainlp.summarize import extract_keywords\n",
    "from pythainlp.summarize import summarize\n",
    "import itertools\n",
    "import sqlite3\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from pythainlp.tag import tag_provinces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url, depth,visited):\n",
    "    if depth < 2 :\n",
    "        visited.add(url)\n",
    "        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "        #time.sleep(0.25)\n",
    "        response = requests.get(url,headers=headers)\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "        links = soup.find_all('a')\n",
    "        links = [link.get('href') for link in links if link.get('href') and not link.get('href').startswith('#')]\n",
    "        links = [urljoin(url, link) for link in links if link]\n",
    "\n",
    "        for link in links:\n",
    "            if link not in visited:\n",
    "                visited.add(link)\n",
    "                #if link.startswith(url):\n",
    "                crawl(link,depth=depth+1, visited=visited)\n",
    "    return visited\n",
    "base_url = 'https://news.kapook.com'\n",
    "website_dict3 = crawl(base_url, 0, set())\n",
    "print(website_dict3)\n",
    "print(len(website_dict3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_domain(base_url,links):\n",
    "    result= set()\n",
    "    for link in links :\n",
    "        if link.startswith(base_url):\n",
    "            result.add(link)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "domain = check_domain(base_url,website_dict3)\n",
    "print(len(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_domain(base_url,links):\n",
    "    result= set()\n",
    "    for link in links :\n",
    "        if not link.startswith(base_url):\n",
    "            result.add(link)\n",
    "    return result\n",
    "\n",
    "not_domain = check_not_domain(base_url,website_dict3)\n",
    "print(not_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_links= {'https://www.thairath.co.th':0,'https://news.kapook.com':0}\n",
    "def check_ref(base,links):\n",
    "    for i in links:\n",
    "        for j in base:\n",
    "            if i.startswith(j):\n",
    "                target_links[base_url]+=1\n",
    "    return target_links\n",
    "\n",
    "ref=check_ref(target_links,not_domain)\n",
    "print(ref)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thai:\n",
    "    def __init__(self,data:list):\n",
    "        self.data_value = data\n",
    "        self.sentence = self.get_sentence()\n",
    "        self.keyword = self.get_keyword()\n",
    "        self.summarize = self.get_summarize()\n",
    "    def make_sentence(self,list_word):\n",
    "        self.sentence_value = ''\n",
    "        for i in list_word:\n",
    "            for i in list_word:\n",
    "                if pythainlp.util.countthai(i)<30:\n",
    "                    list_word.remove(i)\n",
    "        self.sentence_value = ' '.join(list_word)\n",
    "        return self.sentence_value\n",
    "    def get_sentence(self):\n",
    "        self.sentence_result = self.make_sentence(self.data_value)\n",
    "        self.sentence_result = unicodedata.normalize(\"NFKD\", self.sentence_result)\n",
    "        return self.sentence_result\n",
    "    def get_keyword(self):\n",
    "        self.keyword_result = {}\n",
    "        self.keyword_value = self.get_tokenize()\n",
    "        try:\n",
    "            self.keyword_dict = find_keyword(self.keyword_value)\n",
    "        # Iterate over the keys in the dictionary\n",
    "            for key in self.keyword_dict:\n",
    "        # Check if the key is text (i.e., not a space or quotation mark)\n",
    "                if key.isalpha():\n",
    "            # If the key is text, add it to the new dictionary\n",
    "                    self.keyword_result[key] = self.keyword_dict[key]\n",
    "            return self.keyword_result\n",
    "        except:\n",
    "             self.keyword_result['NONE'] = 0\n",
    "             return self.keyword_result\n",
    "        \n",
    "    def get_summarize(self):\n",
    "        self.summarize_result =[]\n",
    "        self.summarize_result = summarize(self.sentence,n=10)\n",
    "        return self.summarize_result\n",
    "    def get_tokenize(self):\n",
    "        self.word_token = word_tokenize(self.sentence, engine=\"newmm\")\n",
    "        return self.word_token\n",
    "    def get_location(self):\n",
    "        self.location_Result = self.location()\n",
    "        self.location_counts = {}\n",
    "        for location, _ in self.location_Result:\n",
    "            if location in self.location_counts:\n",
    "                self.location_counts[location] += 1\n",
    "            else:\n",
    "                self.location_counts[location] = 1\n",
    "        return self.location_counts\n",
    "    def location(self):\n",
    "        self.data = self.get_tokenize()\n",
    "        self.location_value = tag_provinces(self.data)\n",
    "        self.Result_location = filtered_input = [entry for entry in self.location_value if entry[1] == 'B-LOCATION']\n",
    "        return self.Result_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  title_tag = soup.find('title').text\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "  for p in p_tags:\n",
    "    if p.string != None:\n",
    "      p_list.append(unicodedata.normalize(\"NFKD\", p.string))\n",
    "  if len(p_list) == 0:\n",
    "    p_list.append('ไม่พบข้อความในเว็บนี้')\n",
    "  if  title_tag == None:\n",
    "    title_tag = 'None'\n",
    "  p_tag = \"\".join(p_list)\n",
    "    \n",
    "  return p_tag, title_tag,p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  title_tag = soup.find('title').text\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "\n",
    "  return p_tags\n",
    "  \n",
    "url =  'https://www.thairath.co.th/news/foreign/2628056'\n",
    "scrape_tags(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  title_tag = soup.find('title').text\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "  for p in p_tags:\n",
    "    if p.string != None:\n",
    "      p_list.append(unicodedata.normalize(\"NFKD\", p.string))\n",
    "  if len(p_list) == 0:\n",
    "    p_list.append('ไม่พบข้อความในเว็บนี้')\n",
    "  \n",
    "  p_tag = \"\".join(p_list)\n",
    "  thai_nlp = Thai(p_list)\n",
    "  keyword = thai_nlp.keyword\n",
    "  keyword = {k: v for k, v in sorted(keyword.items(), key=lambda item: item[1], reverse=True)}\n",
    "  keyword = dict(itertools.islice(keyword.items(), 5))\n",
    "  summarize_article = thai_nlp.summarize\n",
    "  try:\n",
    "    location = 'จ.'+max(thai_nlp.get_location().keys())\n",
    "  except:\n",
    "    location = 'NONE'\n",
    "  \n",
    "  if  title_tag == None:\n",
    "    title_tag = summarize_article[0]\n",
    "    \n",
    "  return p_tag, title_tag, keyword,location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=dict()\n",
    "for i in domain:\n",
    "    doc = []\n",
    "    p_tag, title_tag, keyword,location = scrape_tags(i)\n",
    "    doc.append(i)\n",
    "    doc.append(p_tag)\n",
    "    doc.append(title_tag)\n",
    "    doc.append(location)\n",
    "    lk = list(keyword)\n",
    "    for i in lk:\n",
    "        if i not in d:\n",
    "            d[i]=list()\n",
    "        d[i].extend(doc)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tags(url):\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  title_tag = soup.find('title').text\n",
    "  p_tags = soup.find_all('p')\n",
    "  p_list =[]\n",
    "  for p in p_tags:\n",
    "    if p.string != None:\n",
    "      p_list.append(unicodedata.normalize(\"NFKD\", p.string))\n",
    "  if len(p_list) == 0:\n",
    "    p_list.append('ไม่พบข้อความในเว็บนี้')\n",
    "  \n",
    "  p_tag = \"\".join(p_list)\n",
    "  thai_nlp = Thai(p_list)\n",
    "  keyword = thai_nlp.keyword\n",
    "  keyword = {k: v for k, v in sorted(keyword.items(), key=lambda item: item[1], reverse=True)}\n",
    "  keyword = dict(itertools.islice(keyword.items(), 5))\n",
    "  summarize_article = thai_nlp.summarize\n",
    "  try:\n",
    "    location = 'จ.'+max(thai_nlp.get_location().keys())\n",
    "  except:\n",
    "    location = 'NONE'\n",
    "  \n",
    "  if  title_tag == None:\n",
    "    title_tag = summarize_article[0]\n",
    "    \n",
    "  return p_tag, title_tag, keyword,location\n",
    "\n",
    "def insert_data_to_db(website, p_tag, title, keyword,location):\n",
    "  \n",
    "  for i in keyword:\n",
    "    conn.execute(\"INSERT INTO DATA5 (WEBSITE, BODY, TITLE, KEYWORD, WORD_FREQUENCY ,LOCATION) VALUES (?, ?, ?, ?, ?, ?)\", (website, p_tag, title, i, keyword[i],location))\n",
    "    conn.commit()\n",
    "    print(f'For website {website}')\n",
    "    print(f'For the p tags is:{p_tag}') \n",
    "    print(f'For the title tag is: {title}')\n",
    "    print(f'For the keyword is:{i}')\n",
    "    print(f'Forthe word frequency is:{keyword[i]}')\n",
    "    print(f'For the location is:{location}')\n",
    "\n",
    " \n",
    "\n",
    "# Iterate through the websites\n",
    "conn = sqlite3.connect('scraped_data.db')\n",
    "conn.execute('''CREATE TABLE DATA5\n",
    "             (ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             WEBSITE STRING NOT NULL,\n",
    "             BODY TEXT NOT NULL,\n",
    "             TITLE TEXT NOT NULL,\n",
    "             KEYWORD TEXT NOT NULL,\n",
    "             WORD_FREQUENCY INT NOT NULL,\n",
    "             LOCATION TEXT );''')\n",
    "j=0\n",
    "for website in domain:\n",
    "  p_tag, title, keyword,location = scrape_tags(website)\n",
    "  insert_data_to_db(website, p_tag, title, keyword,location)\n",
    "  j+=1\n",
    "  print(j)\n",
    "       \n",
    "\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f77b640396ee907328d3f2b1dbf7d1073670d4b49ce003bfc4c9fbcbcb50868c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
